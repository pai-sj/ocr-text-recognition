{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run this script in colab\n",
    "# !git clone https://github.com/pai-sj/ocr-text-recognition.git\n",
    "# import sys\n",
    "# sys.path.append(\"./ocr-text-recognition/\")\n",
    "# !pip install -r ./ocr-text-recognition/requirements.txtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import tensorflow as tf \n",
    "assert int(tf.__version__[:1]) < 2.0, \"해당 코드는 1.x에서만 동작합니다.\"\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ MNIST 데이터셋 \\]\n",
    "\n",
    "MNIST 데이터셋을 통해 정상적으로 동작하는지를 확인해보도록 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 가져오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import SerializationDataset\n",
    "\n",
    "train_set = SerializationDataset('mnist','train',\n",
    "                                 digit=5,pad_range=(3,10))\n",
    "validation_set = SerializationDataset('mnist','validation',\n",
    "                                      digit=5,pad_range=(3,10))\n",
    "test_set = SerializationDataset('mnist','test',\n",
    "                                digit=(3,8),pad_range=(3,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 Generator 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.generator import Seq2SeqGenerator\n",
    "\n",
    "train_gen = Seq2SeqGenerator(train_set, \n",
    "                          batch_size=32)\n",
    "valid_gen = Seq2SeqGenerator(validation_set, \n",
    "                          batch_size=100, \n",
    "                          shuffle=False)\n",
    "test_gen = Seq2SeqGenerator(test_set, \n",
    "                         batch_size=500, \n",
    "                         shuffle=False)\n",
    "\n",
    "conv2text = test_gen.convert2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = train_gen[0]\n",
    "for i in range(3):\n",
    "    image = X['images'][i,:,:,0]\n",
    "    dec_input = X['decoder_inputs'][i]\n",
    "    dec_input = conv2text(dec_input)\n",
    "    \n",
    "    output = Y['output_seqs'][i]\n",
    "    output = conv2text(output)\n",
    "\n",
    "    plt.title(f\"Decoder Input : {dec_input} \\n Model Output : {output}\")\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CRNN과 다른 데이터셋이 필요합니다. CRNN과 달리, Images, Decoder Input, Model Output 이렇게 총 3개의 데이터가 필요합니다. Decoder Input과 Model Output은 1번의 Time Step 만큼 차이가 납니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ 2. SRN(Sequence Recognition Network) \\]\n",
    "\n",
    "MNIST 데이터셋을 통해 `SRN(Sequence Recognition Network)`을 학습시켜보도록 하겠습니다. SRN은 CRNN의 구조와 Seq2Seq, 그리고 Attention Network을 합친 모델입니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/M11craN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CRNN과 동일한 부분들 구성하기\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Source Features 계산하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/nDZbuC2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.layers import ConvFeatureExtractor, Map2Sequence\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 28\n",
    "num_classes = 10\n",
    "n_conv = 16\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# For Gray Scale Image & Dynamic width\n",
    "inputs = Input(shape=(height, None, 1),name='images')\n",
    "\n",
    "# n_conv == Convolution Filter의 갯수를 정하는 계수 F\n",
    "conv_maps = ConvFeatureExtractor(n_conv,\n",
    "                                 name='feature_extractor')(inputs)\n",
    "feature_seqs = Map2Sequence(name='map_to_sequence')(conv_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.F) `Map2Sequence`의 역할\n",
    "\n",
    "Conv_maps의 Shape을 변경하여, Bidirectional LSTM Layer의 Input Shape 형태로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"conv_maps의 shape : {conv_maps.shape}\")\n",
    "print(f\"feature_seqs의 shape : {feature_seqs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Encoder State Vector($S_{encoder}$) 계산하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/kgZLw3N.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "states_{encoder} = [H_{forward} ; H_{backward}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM\n",
    "\n",
    "class BLSTMEncoder(Layer):\n",
    "    \"\"\"\n",
    "    CRNN 중 Recurrent Layers에 해당하는 Module Class\n",
    "    Convolution Layer의 Image Feature Sequence를 Encoding하여,\n",
    "    우리가 원하는 Text Feature Sequence로 만듦\n",
    "\n",
    "    | Layer Name | #Hidden Units |\n",
    "    | ----       | ------ |\n",
    "    | Bi-LSTM1   | 256    |\n",
    "    | Bi-LSTM2   | 256    |\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_units=256, **kwargs):\n",
    "        self.n_units = n_units\n",
    "        super().__init__(**kwargs)\n",
    "        self.lstm1 = Bidirectional(LSTM(n_units, return_sequences=True))\n",
    "        self.lstm2 = Bidirectional(LSTM(n_units, return_sequences=True))\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.lstm1(inputs)\n",
    "        x = self.lstm2(x)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"n_units\": self.n_units\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lstm = 256\n",
    "states_encoder = BLSTMEncoder(n_units=n_lstm)(feature_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'states_encoder의 shape : {states_encoder.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bidirectional 이므로, Forward 방향에서의 state, Backward 방향에서의 state가 모였기 때문에 n_lstm의 2배만큼 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attention 구성하기\n",
    "----\n",
    "\n",
    "![Imgur](https://i.imgur.com/clo5uEw.png)\n",
    "\n",
    "Attention은 우리가 필요한 정보만을 취사선택할 수 있도록 만든 모듈입니다. 글자 영상 추출기를 통해 만들어진 정보 중 Decoder에서 필요한 정보만을 취사선택할 수 있도록 만듭니다. Attention은 하나의 방법론으로, 다양한 형태로 구성할 수 있습니다. 이번에 쓰는 방법은 Luong Attention입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Score 계산하기\n",
    "![Imgur](https://i.imgur.com/vNkwyPs.png)\n",
    "\n",
    "Decoder의 정보중 어떤 정보가 더 중요한 정보인지를 판단하기 위한 지표로, Score을 아래와 같이 계산합니다. 내적의 연산의 중요한 특징은, 두 벡터가 유사할수록 그 크기가 커진다는 점에 있습니다. 디코더의 벡터($S$)와 인코더의 벡터($V$)를 내적해줌으로써, 디코더와 가까운 정보에게 더 가중치를 주게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Score를 Normalize 하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/hWCD9fK.png)\n",
    "\n",
    "각 Time Step 별로 점수가 따로 매겨지게 됩니다. 이를 합산할 때, 그 크기가 지나치게 커지지 않도록, 전체 score의 합이 1이 되도록 표준화합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Context Vector 계산하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/OOvZyzv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 Encoder 정보와 Score 점수를 곱해서 나온 값이 바로 Context Vector가 됩니다. 이 정보는 Encoder의 정보 중 필요한 정보만을 추출한 정보가 됩니다. 이 정보를 바탕으로 분류기에 넣으면 우리가 원하는 철자 정보를 얻을 수 있게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Softmax\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotAttention(Layer):\n",
    "    \"\"\" General Dot-Product Attention Network (Luong, 2015)\n",
    "\n",
    "    * n_state :\n",
    "       if n_state is None, Dot-Product Attention(s_t * h_i)\n",
    "       if n_state is number, general Dot-Product Attention(s_t * W_a * h_i)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_state=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_state = n_state\n",
    "        if isinstance(self.n_state, int):\n",
    "            self.key_dense = Dense(self.n_state)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        states_encoder = inputs[0]\n",
    "        states_decoder = inputs[1]\n",
    "\n",
    "        # (0) adjust the size of encoder state to the size of decoder state\n",
    "        if isinstance(self.n_state, int):\n",
    "            key_vector = self.key_dense(states_encoder)\n",
    "        else:\n",
    "            key_vector = states_encoder\n",
    "\n",
    "        # (1) Calculate Score\n",
    "        expanded_states_encoder = key_vector[:, None, ...]\n",
    "        # >>> (batch size, 1, length of encoder sequence, num hidden)\n",
    "        expanded_states_decoder = states_decoder[..., None, :]\n",
    "        # >>> (batch size, length of decoder sequence, 1, num hidden)\n",
    "        score = K.sum(expanded_states_encoder * expanded_states_decoder,\n",
    "                      axis=-1)\n",
    "        # >>> (batch size, length of decoder input, length of encoder input)\n",
    "        # (2) Normalize score\n",
    "        attention = Softmax(axis=-1, name='attention')(score)\n",
    "\n",
    "        # (3) Calculate Context Vector\n",
    "        value_vector = states_encoder[:, None, ...] # Key Vector와 Value Vector을 다르게 둚\n",
    "        context = K.sum(value_vector * attention[..., None], axis=2)\n",
    "        # >>> (batch size, length of decoder input, num hidden)\n",
    "\n",
    "        return context, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Decoder 구성하기\n",
    "\n",
    "어텐션에 Query를 던질 Decoder을 구현해보도록 하겠습니다. 글자를 임베딩하는 Embedding Layer와 GRUCell을 이용하도록 하겠습니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/f0jLCf5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Gray Scale Image & Dynamic width\n",
    "n_embed = 3\n",
    "decoder_inputs = Input(shape=(None,),name='decoder_inputs')\n",
    "\n",
    "embedding_layer = Embedding(num_classes+1, n_embed)\n",
    "embeded_decoder_inputs = embedding_layer(decoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/yArrBKh.png)\n",
    "\n",
    "우리는 초기 state를 넣는 인자를 따로 만들었습니다.<br>\n",
    "이후에 inference Logic을 짤 때, Decoder Logic에서 필요하므로 추가하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Attention Layer 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Softmax\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        states_encoder = inputs[0]\n",
    "        states_decoder = inputs[1]\n",
    "        \n",
    "        # (1) Calculate Score\n",
    "        expanded_states_encoder = states_encoder[:,None,...] \n",
    "        # >>> (batch size, 1, length of encoder sequence, num hidden)\n",
    "        expanded_states_decoder = states_decoder[...,None,:] \n",
    "        # >>> (batch size, length of decoder sequence, 1, num hidden)            \n",
    "        score = K.sum(expanded_states_encoder * expanded_states_decoder,\n",
    "                          axis=-1)\n",
    "        # >>> (batch size, length of decoder input, length of encoder input)\n",
    "        \n",
    "        # (2) Normalize score\n",
    "        attention = Softmax(axis=-1, name='attention')(score)\n",
    "        \n",
    "        # (3) Calculate Context Vector\n",
    "        context = K.sum(expanded_states_encoder * attention[...,None], axis=2)\n",
    "        # >>> (batch size, length of decoder input, num hidden)\n",
    "        \n",
    "        return context, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotattend = DotAttention()\n",
    "\n",
    "context, attention = dotattend([states_encoder, states_decoder])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) prediction with Softmax\n",
    "\n",
    "![Imgur](https://i.imgur.com/ihz1Hpq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clf = 256\n",
    "\n",
    "clf1_layer = Dense(n_clf, activation='relu')\n",
    "clf2_layer = Dense(num_classes+1, \n",
    "                   activation='softmax',\n",
    "                   name='output_seqs')\n",
    "\n",
    "concat_output = concatenate([context, states_decoder],\n",
    "                            name='concat_output')\n",
    "fc_outputs = clf1_layer(concat_output)\n",
    "predictions = clf2_layer(fc_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 전체 모델 구성하고 학습시키기\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "height = 28\n",
    "num_classes = 10\n",
    "n_conv = 16 # the number of Convolution filter\n",
    "n_state = 128 # the number of BLSM units\n",
    "n_embed = 10 # the size of embedding vector\n",
    "n_clf = 128 # the number of units in classifier Dense layer\n",
    "\n",
    "# Image Encoder\n",
    "inputs = Input(shape=(height, None, 1),name='images')\n",
    "conv_maps = ConvFeatureExtractor(n_conv,\n",
    "                                 name='feature_extractor')(inputs)\n",
    "feature_seqs = Map2Sequence(name='map_to_sequence')(conv_maps)\n",
    "states_encoder = Dense(n_state, activation='tanh')(feature_seqs)\n",
    "#states_encoder = BLSTMEncoder(n_units=n_lstm)(feature_seqs)    \n",
    "\n",
    "# Embedding Layer\n",
    "decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n",
    "embedding_layer = Embedding(num_classes+1, n_embed)\n",
    "embedding_target = embedding_layer(decoder_inputs)\n",
    "\n",
    "# Text Decoder\n",
    "decoder_state_inputs = Input(shape=(n_state,), name='decoder_state')\n",
    "gru_layer = GRU(n_state, \n",
    "                name='decoder_gru', \n",
    "                return_sequences=True)\n",
    "states_decoder = gru_layer(embedding_target,\n",
    "                           initial_state=decoder_state_inputs)\n",
    "\n",
    "# Attention Layer\n",
    "dotattend = DotAttention()\n",
    "context, attention = dotattend([states_encoder, states_decoder])\n",
    "\n",
    "# Classifier Layer\n",
    "clf1_layer = Dense(n_clf, activation='relu')\n",
    "clf2_layer = Dense(num_classes+1, activation='softmax',name='output_seqs')\n",
    "\n",
    "concat_output = concatenate([context, states_decoder], name='concat_output')\n",
    "fc_outputs = clf1_layer(concat_output)\n",
    "predictions = clf2_layer(fc_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (8) 추론 모델과 학습 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training\n",
    "trainer = Model([inputs, \n",
    "                 decoder_inputs,\n",
    "                 decoder_state_inputs], \n",
    "                predictions, name='trainer')\n",
    "\n",
    "# For Inference\n",
    "# - (1) Encoder\n",
    "encoder = Model(inputs, states_encoder, \n",
    "                name='encoder')\n",
    "\n",
    "# - (2) Decoder\n",
    "states_encoder_input = Input((None,n_state), \n",
    "                             name='states_encoder_input')\n",
    "\n",
    "context, attention = dotattend([states_encoder_input, states_decoder])\n",
    "concat_output = concatenate([context, states_decoder], axis=-1, \n",
    "                            name='concat_output')\n",
    "fc_outputs = clf1_layer(concat_output)\n",
    "predictions = clf2_layer(fc_outputs)\n",
    "\n",
    "decoder = Model([states_encoder_input, decoder_inputs, decoder_state_inputs], \n",
    "                [states_decoder, predictions], name='decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (9) 학습 모델 Compile하기\n",
    "\n",
    "학습할 모델에 대한 Loss Function와 optimizer를 결정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masking_sparse_categorical_crossentropy(mask_value):\n",
    "    \"\"\"\n",
    "    Runs sparse Categorical Crossentropy Loss Algorithm on each batch element Without Masking Value\n",
    "\n",
    "    :param mask_value: masking value for preventing Back Propagation\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, K.floatx())\n",
    "        mask = K.equal(y_true, mask_value)\n",
    "        mask = 1 - K.cast(mask, K.floatx())\n",
    "        y_true = y_true * mask\n",
    "\n",
    "        loss = K.sparse_categorical_crossentropy(y_true, y_pred) * mask\n",
    "        return K.sum(loss) / K.sum(mask)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "\n",
    "trainer.compile(Adam(lr=1e-3),\n",
    "                loss={\"output_seqs\":masking_sparse_categorical_crossentropy(-1)},\n",
    "                target_tensors=[y_true])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10) 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks =[]\n",
    "rlrop = ReduceLROnPlateau(\n",
    "    factor=0.5, patience=5, \n",
    "    min_lr=1e-6, verbose=1)\n",
    "callbacks.append(rlrop)\n",
    "\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = Seq2SeqGenerator(train_set, \n",
    "                          batch_size=32,\n",
    "                          state_size=n_state)\n",
    "valid_gen = Seq2SeqGenerator(validation_set, \n",
    "                             batch_size=100,\n",
    "                             shuffle=False,\n",
    "                             state_size=n_state)                            \n",
    "test_gen = Seq2SeqGenerator(test_set, \n",
    "                            batch_size=500, \n",
    "                            shuffle=False,\n",
    "                            state_size=n_state)                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = trainer.fit_generator(train_gen,\n",
    "                             epochs=epochs,\n",
    "                             validation_data=valid_gen,\n",
    "                             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (11) 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,_ = test_gen[0]\n",
    "\n",
    "# Target image \n",
    "target_images = X['images'][:10]\n",
    "\n",
    "# Encoder 결과 계산\n",
    "states_encoder_ = encoder.predict(target_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "batch_size = target_images.shape[0]\n",
    "\n",
    "prev_inputs = np.ones((batch_size,1)) * EOS_TOKEN\n",
    "prev_states = np.zeros((batch_size, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = prev_inputs.copy()\n",
    "while True:\n",
    "    states_decoder_, predictions_ = decoder.predict({\n",
    "        \"states_encoder_input\" : states_encoder_,\n",
    "        \"decoder_inputs\": prev_inputs,\n",
    "        \"decoder_state\": prev_states        \n",
    "    })\n",
    "    prev_states = states_decoder_[:,-1,:]\n",
    "    prev_inputs = np.argmax(predictions_,axis=-1)\n",
    "    \n",
    "    if np.all(prev_inputs == EOS_TOKEN):\n",
    "        break\n",
    "    result = np.concatenate([result,prev_inputs],axis=-1)\n",
    "result = result[:,1:].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, seq in zip(target_images,result):\n",
    "    plt.title(seq)\n",
    "    plt.imshow(image[:,:,0])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
