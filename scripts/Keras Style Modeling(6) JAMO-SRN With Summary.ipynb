{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from functools import partial\n",
    "\n",
    "from utils.dataset import OCRDataset\n",
    "from models.generator import JAMOSeq2SeqGenerator\n",
    "from models.jamo import compose_unicode\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "import tensorflow as tf \n",
    "assert int(tf.__version__[:1]) < 2.0, \"해당 코드는 1.x에서만 동작합니다.\"\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글 Matplotlib 출력 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 나눔 폰트를 다운받기\n",
    "!apt-get update -qq\n",
    "!apt-get install fonts-nanum* -qq\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "# 2. 나눔 폰트의 위치 가져오기 \n",
    "system_font = fm.findSystemFonts() # 현재 시스템에 설치된 폰트\n",
    "nanum_fonts = [font for font in system_font if \"NanumBarunGothic.ttf\" in font]\n",
    "font_path = nanum_fonts[0] # 설정할 폰트의 경로\n",
    "\n",
    "# 3. 나눔 폰트로 설정하기\n",
    "font_name = fm.FontProperties(fname=font_path, size=10).get_name()\n",
    "plt.rc(\"font\",family=font_name)\n",
    "\n",
    "# 4. 폰트 재설정하기\n",
    "fm._rebuild()\n",
    "\n",
    "# 5. (optional) minus 기호 깨짐 방지\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ OCR 데이터셋 - SRN ver3.0 \\]\n",
    "\n",
    "MNIST 데이터셋을 통해 `SRN(Sequence Recognition Network)`을 학습시켜보도록 하겠습니다. SRN은 CRNN의 구조와 Seq2Seq, 그리고 Attention Network을 합친 모델입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 가져오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 최소/최대 길이\n",
    "min_words = 4\n",
    "max_words = 8\n",
    "\n",
    "df = pd.read_csv(\"../datasets/wordslist.txt\",names=['word'])\n",
    "df = df.drop_duplicates()\n",
    "df = df[df.word.str.match(r'^[가-힣]+$')]\n",
    "df = df[\n",
    "    df.word.map(\n",
    "        lambda x: (len(x) >= min_words) \n",
    "        and (len(x) <= max_words))]\n",
    "words = df.word.values\n",
    "np.random.shuffle(words)\n",
    "words = np.append(words,\"정보화진흥원\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCRDataset setting\n",
    "OCRDataset = partial(OCRDataset,\n",
    "                     font_size=24,\n",
    "                     bg_noise=0.2,\n",
    "                     affine_noise=(0.0,0.01),\n",
    "                     normalize=False,\n",
    "                     random_shift=False,                     \n",
    "                     color_noise=(0.1,0.3),\n",
    "                     gray_scale=True)\n",
    "\n",
    "dataset = OCRDataset(words)\n",
    "images, labels = dataset[0:3]\n",
    "\n",
    "for image, label in zip(images, labels):\n",
    "    plt.title(label)\n",
    "    plt.imshow(image[:,:,0],cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 Generator 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 군을 기준으로 나누기 \n",
    "# validation words는 5%만 둚\n",
    "valid_words = words[:len(words)*5//100]\n",
    "train_words = words[len(words)*5//100:]\n",
    "train_words = train_words\n",
    "\n",
    "valid_set = OCRDataset(valid_words)\n",
    "train_set = OCRDataset(train_words)\n",
    "\n",
    "train_gen = JAMOSeq2SeqGenerator(train_set, batch_size=64)\n",
    "valid_gen = JAMOSeq2SeqGenerator(valid_set, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = train_gen[0]\n",
    "for i in range(3):\n",
    "    image = X['images'][i,:,:,0]\n",
    "    plt.title(compose_unicode(Y['output_seqs'][i])[0])\n",
    "    plt.imshow(image,cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 모델을 학습할 때에는 Images, Decoder Input, Model Output 이렇게 총 3개의 데이터가 필요합니다. Decoder input과 Model Output은 1번의 Time Step 만큼 차이가 납니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델 구성하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.layers import Map2Sequence, ResidualConvFeatureExtractor\n",
    "from models.layers import BLSTMEncoder, CTCDecoder, DotAttention\n",
    "from models.layers import JamoEmbedding, JamoCompose, JamoClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarySequence(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.summary_conv_1 = Conv1D(units, 3, 2,\n",
    "                                     activation='relu',\n",
    "                                     padding='same')\n",
    "        self.summary_conv_2 = Conv1D(units, 3, 2,\n",
    "                                     activation='relu',\n",
    "                                     padding='same')\n",
    "        self.summary_conv_3 = Conv1D(units, 3, 2,\n",
    "                                     activation='relu',\n",
    "                                     padding='same')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.summary_conv_1(inputs)\n",
    "        x = self.summary_conv_2(x)\n",
    "        x = self.summary_conv_3(x)        \n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"units\": self.units\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "height = 36\n",
    "\n",
    "n_conv = 16 # the number of Convolution filter\n",
    "n_state = 128 # the number of GRU units\n",
    "n_embed = 16 # the size of embedding vector\n",
    "n_summary = 64 # the size of summary vector\n",
    "n_clf = 256 # the number of units in classifier Dense layer\n",
    "\n",
    "# Image Encoder\n",
    "inputs = Input(shape=(height, None, 1),name='images')\n",
    "conv_maps = ResidualConvFeatureExtractor(n_conv,\n",
    "                                         name='feature_extractor')(inputs)\n",
    "feature_seqs = Map2Sequence(name='map_to_sequence')(conv_maps)\n",
    "\n",
    "# Image Summary\n",
    "summary_seqs = SummarySequence(n_summary)(feature_seqs)\n",
    "summary_gru_layer = GRU(n_state, \n",
    "                name='summary_gru',\n",
    "                go_backwards=True,\n",
    "                return_state=True)\n",
    "summary_state = summary_gru_layer(summary_seqs)[0]\n",
    "\n",
    "# Embedding Layer\n",
    "decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n",
    "jamo_embedding_layer = JamoEmbedding(n_embed)\n",
    "jamo_embedding = jamo_embedding_layer(decoder_inputs)\n",
    "\n",
    "# Text Decoder\n",
    "#decoder_state_inputs = Input(shape=(n_state,), name='decoder_state')\n",
    "gru_layer = GRU(n_state, \n",
    "                name='decoder_gru', \n",
    "                return_sequences=True)\n",
    "states_decoder = gru_layer(jamo_embedding,\n",
    "                           initial_state=summary_state)\n",
    "\n",
    "# Attention Layer\n",
    "dotattend = DotAttention(n_state=n_state)\n",
    "context, attention = dotattend([feature_seqs, states_decoder])\n",
    "\n",
    "concat_state = concatenate([context, states_decoder],\n",
    "                            axis=-1, name='concat_output')\n",
    "# Classification Layer\n",
    "jamo_clf = JamoClassifier(n_clf, name='output_seqs')\n",
    "\n",
    "predictions = jamo_clf(concat_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (8) 추론 모델과 학습 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training\n",
    "trainer = Model([inputs, \n",
    "                 decoder_inputs], \n",
    "                 predictions, name='trainer')\n",
    "\n",
    "# For Inference\n",
    "# - (1) Encoder\n",
    "encoder = Model(inputs, [feature_seqs,summary_state], \n",
    "                name='encoder')\n",
    "\n",
    "# - (2) Decoder\n",
    "num_feature = feature_seqs.shape.as_list()[-1]\n",
    "states_encoder_input = Input((None, num_feature),\n",
    "                             name='states_encoder_input')\n",
    "decoder_state_inputs = Input(shape=(n_state,), name='decoder_state')\n",
    "\n",
    "states_decoder = gru_layer(jamo_embedding,\n",
    "                           initial_state=decoder_state_inputs)\n",
    "\n",
    "context, attention = dotattend([states_encoder_input, states_decoder])\n",
    "concat_state = concatenate([context, states_decoder], axis=-1, \n",
    "                            name='concat_output')\n",
    "predictions = jamo_clf(concat_state)\n",
    "output_decoder = JamoCompose(name='jamocompose')(predictions)\n",
    "\n",
    "decoder = Model([states_encoder_input,\n",
    "                 decoder_inputs,\n",
    "                 decoder_state_inputs], \n",
    "                [states_decoder, output_decoder], name='decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (9) 학습 모델 Compile하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.optimizer import AdamW\n",
    "from models.losses import jamo_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "\n",
    "trainer.compile(AdamW(lr=1e-2),\n",
    "                loss={\n",
    "                    \"output_seqs\":jamo_categorical_crossentropy,\n",
    "                },\n",
    "                target_tensors=[y_true])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10) 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "callbacks =[]\n",
    "rlrop = ReduceLROnPlateau(\n",
    "    factor=0.5, patience=2, \n",
    "    min_lr=1e-6, verbose=1)\n",
    "callbacks.append(rlrop)\n",
    "\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = JAMOSeq2SeqGenerator(train_set, batch_size=32,\n",
    "                                 return_initial_state=False,\n",
    "                                 state_size=n_state)\n",
    "valid_gen = JAMOSeq2SeqGenerator(valid_set, batch_size=32,\n",
    "                                 return_initial_state=False,                                 \n",
    "                                 state_size=n_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = trainer.fit_generator(train_gen,\n",
    "                             steps_per_epoch=1000,                             \n",
    "                             epochs=epochs,\n",
    "                             validation_data=valid_gen,\n",
    "                             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (11) 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = train_gen.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,_ = test_gen[0]\n",
    "\n",
    "# Target image \n",
    "target_images = X['images'][:10]\n",
    "\n",
    "# Encoder 결과 계산\n",
    "states_encoder_ = encoder.predict(target_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "batch_size = target_images.shape[0]\n",
    "\n",
    "prev_inputs = np.ones((batch_size,1)) * EOS_TOKEN\n",
    "prev_states = np.zeros((batch_size, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = prev_inputs.copy()\n",
    "while True:\n",
    "    states_decoder_, predictions_ = decoder.predict({\n",
    "        \"states_encoder_input\" : states_encoder_,\n",
    "        \"decoder_inputs\": prev_inputs,\n",
    "        \"decoder_state\": prev_states        \n",
    "    })\n",
    "    prev_states = states_decoder_[:,-1,:]\n",
    "    prev_inputs = np.argmax(predictions_,axis=-1)\n",
    "    \n",
    "    if np.all(prev_inputs == EOS_TOKEN):\n",
    "        break\n",
    "    result = np.concatenate([result,prev_inputs],axis=-1)\n",
    "result = result[:,1:].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, seq in zip(target_images,result):\n",
    "    plt.title(seq)\n",
    "    plt.imshow(image[:,:,0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reference : \n",
    "1. [Attention based Seq2Seq in Keras](https://wanasit.github.io/attention-based-sequence-to-sequence-in-keras.html)\n",
    "2. [](https://medium.com/datalogue/attention-in-keras-1892773a4f22)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
