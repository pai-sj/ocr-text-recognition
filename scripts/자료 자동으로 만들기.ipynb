{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # Font로 자동으로 이미지 만들기\n",
    "    import cairocffi as cairo\n",
    "except:\n",
    "    !pip install cairocffi\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib\n",
    "from imgaug import augmenters as iaa\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from scipy import ndimage\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 Matplotlib 출력 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 나눔 폰트를 다운받기\n",
    "!apt-get update -qq\n",
    "!apt-get install fonts-nanum* -qq\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "# 2. 나눔 폰트의 위치 가져오기 \n",
    "system_font = fm.findSystemFonts() # 현재 시스템에 설치된 폰트\n",
    "nanum_fonts = [font for font in system_font if \"NanumBarunGothic.ttf\" in font]\n",
    "font_path = nanum_fonts[0] # 설정할 폰트의 경로\n",
    "\n",
    "# 3. 나눔 폰트로 설정하기\n",
    "font_name = fm.FontProperties(fname=font_path, size=10).get_name()\n",
    "plt.rc(\"font\",family=font_name)\n",
    "\n",
    "# 4. 폰트 재설정하기\n",
    "fm._rebuild()\n",
    "\n",
    "# 5. (optional) minus 기호 깨짐 방지\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 단어 데이터 셋 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../datasets/wordslist.txt\"):\n",
    "    !wget https://github.com/acidsound/korean_wordlist/raw/master/wordslist.txt\n",
    "    !mv ./wordslist.txt ../datasets/\n",
    "word_df = pd.read_csv('../datasets/wordslist.txt',names=['word'])\n",
    "word_df = word_df.drop_duplicates()\n",
    "word_df = word_df[word_df.word.str.match(r'^[가-힣]+$')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR 데이터셋 구축하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FONT_LIST = [f.name for f in matplotlib.font_manager.fontManager.ttflist]\n",
    "FONT_LIST = list(set([f for f in FONT_LIST if \"Nanum\" in f]))\n",
    "\n",
    "MAX_WORD = 8 # 최대 철자 수\n",
    "FONT_SIZE = 30 # 폰트 크기\n",
    "MAX_HEIGHT = FONT_SIZE + 8 # 이미지의 최대 Height\n",
    "MAX_WIDTH  = FONT_SIZE*MAX_WORD + 8 # 이미지 최대 Width\n",
    "\n",
    "class OCRDataset:\n",
    "    \"\"\"\n",
    "    generate OCR dataset for Text Recognition\n",
    "    \n",
    "    텍스트 Recognition에 필요한 단어집합을 만ㄷ르어주는 것\n",
    "    \n",
    "    :param words : OCR 단어로 생성할 단어 집합들\n",
    "    :param bg_noise : 가우시안 노이즈의 강도 (0.0~0.5)\n",
    "    :param angle_noise : 회전 각 노이즈의 강도\n",
    "    \"\"\"\n",
    "    def __init__(self, words, font_size, bg_noise=0.0, \n",
    "                 affine_noise = (0.0, 0.02)):\n",
    "        self.words = np.array(words)\n",
    "        self.max_word = max([len(word) for word in self.words])\n",
    "        self.font_size = font_size\n",
    "        self.bg_noise = np.clip(bg_noise,0.,0.5)\n",
    "        self.aug = iaa.PiecewiseAffine(scale=affine_noise)\n",
    "        self.paint_text = lambda word : paint_text(word, \n",
    "                                                   self.max_word,\n",
    "                                                   self.font_size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # 전체 데이터 셋\n",
    "        return len(self.words)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        (1) index -> integer일 때, get single item\n",
    "        (2) index -> np.array | slice, get batch items\n",
    "        \"\"\"\n",
    "        #\n",
    "        if isinstance(index, int):\n",
    "            word = self.words[index]\n",
    "            image = self.paint_text(word)\n",
    "            if self.bg_noise > 0:\n",
    "                image = gaussian_noise(image, self.bg_noise)\n",
    "            return image, word\n",
    "        else:\n",
    "            words = self.words[index]\n",
    "            images = []\n",
    "            for word in words:\n",
    "                image = self.paint_text(word)\n",
    "                if self.bg_noise > 0:\n",
    "                    noise = np.random.uniform(0,self.bg_noise)                    \n",
    "                    image = gaussian_noise(image, )\n",
    "                image = self.aug.augment_image(image)\n",
    "                images.append(image)\n",
    "            return np.stack(images), words\n",
    "\n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self.words)\n",
    "        \n",
    "def paint_text(text, max_word=None, font_size=FONT_SIZE):\n",
    "    '''\n",
    "    Text가 그려진 이미지를 만드는 함수\n",
    "    \n",
    "    max\n",
    "    '''\n",
    "    if max_word is None:\n",
    "        # None이면, text의 word 갯수에 맞춰서 생성\n",
    "        max_word = len(text)\n",
    "    h = font_size + 12 # 이미지 높이, font_size + padding\n",
    "    w = font_size * (1+max_word) + 12 # 이미지 폭    \n",
    "    \n",
    "    surface = cairo.ImageSurface(cairo.FORMAT_RGB24, w, h)\n",
    "    with cairo.Context(surface) as context:\n",
    "        context.set_source_rgb(1, 1, 1)  # White\n",
    "        context.paint()\n",
    "        \n",
    "        # Font Style : Random Pick\n",
    "        context.select_font_face(\n",
    "            np.random.choice(FONT_LIST),\n",
    "            cairo.FONT_SLANT_NORMAL,\n",
    "            np.random.choice([cairo.FONT_WEIGHT_BOLD,\n",
    "                              cairo.FONT_WEIGHT_NORMAL]))\n",
    "        \n",
    "        context.set_font_size(font_size)\n",
    "        box = context.text_extents(text)          \n",
    "        border_w_h = (4, 4)\n",
    "        if box[2] > (w - 2 * border_w_h[1]) or box[3] > (h - 2 * border_w_h[0]):\n",
    "            raise IOError(('Could not fit string into image.'\n",
    "                           'Max char count is too large for given image width.'))\n",
    "\n",
    "        # Random Shift을 통해, 이미지 Augmentation\n",
    "        max_shift_x = w - box[2] - border_w_h[0]\n",
    "        max_shift_y = h - box[3] - border_w_h[1]\n",
    "        top_left_x = np.random.randint(0, int(max_shift_x))\n",
    "        top_left_y = np.random.randint(0, int(max_shift_y))\n",
    "        \n",
    "        context.move_to(top_left_x - int(box[0]),\n",
    "                        top_left_y - int(box[1]))\n",
    "        \n",
    "        # Draw Text\n",
    "        rgb = np.random.uniform(0.5,0.9,size=3)\n",
    "        \n",
    "        context.set_source_rgb(*rgb)\n",
    "        context.show_text(text)\n",
    "    \n",
    "    # cairo data format to numpy data format\n",
    "    buf = surface.get_data()\n",
    "    text_image = np.frombuffer(buf, np.uint8)\n",
    "    text_image = text_image.reshape(h,w,4)\n",
    "    text_image = text_image[:,:,:3]\n",
    "    text_image = text_image.astype(np.float32) / 255\n",
    "    \n",
    "    return text_image\n",
    "\n",
    "def gaussian_noise(image,noise=0.1):\n",
    "    '''\n",
    "    이미지에 가우시안 잡음을 넣어주는 함수\n",
    "    Data Augmentation을 적용하기 위함    \n",
    "    '''\n",
    "    image = image + np.random.normal(0, noise, size=image.shape)\n",
    "    return np.clip(image,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_words = 6\n",
    "max_words = 8\n",
    "\n",
    "word_df = word_df[\n",
    "    word_df.word.map(\n",
    "        lambda x: (len(x) >= min_words) \n",
    "        and (len(x) <= max_words))]\n",
    "\n",
    "words = word_df.word.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = OCRDataset(words, 28, 0.05, affine_noise=(0.0,0.01))\n",
    "dataset.shuffle()\n",
    "images, labels  = dataset[0:3]\n",
    "for image, label in zip(images, labels):\n",
    "    plt.title(label)\n",
    "    plt.imshow(image[:,:,0],cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "가 = ord('가')\n",
    "힣 = ord('힣')\n",
    "\n",
    "KOR_CHARS = [chr(idx) for idx in range(가,힣+1)]\n",
    "KOR2IDX = { char : idx for idx, char in enumerate(KOR_CHARS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRGenerator(Sequence):\n",
    "    \"Generates OCR TEXT Recognition Dataset for Keras\"\n",
    "\n",
    "    def __init__(self, dataset, char_list=KOR_CHARS, batch_size=32, shuffle=True):\n",
    "        \"\"\"\n",
    "        Initialization\n",
    "        \n",
    "        param \n",
    "        :param dataset : instance of class 'OCRDataset'\n",
    "        :param char_list : unique character list (for Embedding)\n",
    "        :param batch_size : the number of batch\n",
    "        :param shuffle : whether shuffle dataset or not\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.char_list = char_list\n",
    "        self.char2idx = { char : idx \n",
    "                         for idx, char \n",
    "                         in enumerate(self.char_list)}\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_classes = len(self.char_list) + 1  # With Blank for CTC LOSS\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the number of batches per epoch\"\n",
    "        return len(self.dataset) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generator one batch of dataset\"\n",
    "        images, texts, _ = self.dataset[self.batch_size * index:\n",
    "                                        self.batch_size * (index + 1)]\n",
    "        # label sequence\n",
    "        labels = np.ones([self.batch_size, self.max_length], np.int32)\n",
    "        label_length = np.zeros([self.batch_size, 1], np.int32)\n",
    "        labels *= -1  # EOS Token value : -1\n",
    "        for idx, text in enumerate(texts):\n",
    "            labels[idx, :len(text)] = text2label(text,self.char2idx)\n",
    "            label_length[idx] = len(text)\n",
    "\n",
    "        return images, labels, label_length\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"Updates indexes after each epoch\"\n",
    "        if self.shuffle:\n",
    "            self.dataset.shuffle()\n",
    "            \n",
    "\n",
    "def text2label(text, char2idx=KOR2IDX):\n",
    "    return np.array([char2idx[char] for char in text])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0",
   "language": "python",
   "name": "tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
