{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from utils.dataset import SerializationDataset\n",
    "from models.generator import Seq2SeqGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "assert int(tf.__version__[:1]) < 2.0, \"해당 코드는 1.x에서만 동작합니다.\"\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ 2. MNIST 데이터셋 - SRN(Sequence Recognition Network) \\]\n",
    "\n",
    "MNIST 데이터셋을 통해 `SRN(Sequence Recognition Network)`을 학습시켜보도록 하겠습니다. SRN은 CRNN의 구조와 Seq2Seq, 그리고 Attention Network을 합친 모델입니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/M11craN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 가져오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = SerializationDataset('mnist','train',\n",
    "                                 digit=(3,10),pad_range=(3,10))\n",
    "validation_set = SerializationDataset('mnist','validation',\n",
    "                                      digit=(3,10),pad_range=(3,10))\n",
    "test_set = SerializationDataset('mnist','test',\n",
    "                                digit=(3,8),pad_range=(3,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 Generator 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = Seq2SeqGenerator(train_set, \n",
    "                          batch_size=32)\n",
    "valid_gen = Seq2SeqGenerator(validation_set, \n",
    "                          batch_size=100, \n",
    "                          shuffle=False)\n",
    "test_gen = Seq2SeqGenerator(test_set, \n",
    "                         batch_size=500, \n",
    "                         shuffle=False)\n",
    "\n",
    "conv2text = test_gen.convert2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = train_gen[0]\n",
    "for i in range(3):\n",
    "    image = X['images'][i,:,:,0]\n",
    "    dec_input = X['decoder_inputs'][i]\n",
    "    dec_input = conv2text(dec_input)\n",
    "    \n",
    "    output = Y['output_seqs'][i]\n",
    "    output = conv2text(output)\n",
    "\n",
    "    plt.title(f\"Decoder Input : {dec_input} \\n Model Output : {output}\")\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 모델을 학습할 때에는 Images, Decoder Input, Model Output 이렇게 총 3개의 데이터가 필요합니다. Decoder input과 Model Output은 1번의 Time Step 만큼 차이가 납니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델 구성하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.layers import ConvFeatureExtractor, Map2Sequence \n",
    "from models.layers import BLSTMEncoder, CTCDecoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Source Features 계산하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/nDZbuC2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 28\n",
    "num_classes = 10\n",
    "n_conv = 16\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# For Gray Scale Image & Dynamic width\n",
    "inputs = Input(shape=(height, None, 1),name='images')\n",
    "\n",
    "# (batch size, height, width, channels) \n",
    "# -> (batch size, width, height, channels)\n",
    "transposed = K.permute_dimensions(inputs, (0, 2, 1, 3))\n",
    "\n",
    "# n_conv == Convolution Filter의 갯수를 정하는 계수 F\n",
    "conv_maps = ConvFeatureExtractor(n_conv,\n",
    "                                 name='feature_extractor')(transposed)\n",
    "feature_seqs = Map2Sequence(name='map_to_sequence')(conv_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.F) `Map2Sequence`의 역할\n",
    "\n",
    "Conv_maps의 Shape을 변경하여, Bidirectional LSTM Layer의 Input Shape 형태로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"conv_maps의 shape : {conv_maps.shape}\")\n",
    "print(f\"feature_seqs의 shape : {feature_seqs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Encoder State Vector($S_{encoder}$) 계산하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/kgZLw3N.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "states_{encoder} = [H_{forward} ; H_{backward}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lstm = 256\n",
    "states_encoder = BLSTMEncoder(n_units=n_lstm)(feature_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'states_encoder의 shape : {states_encoder.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bidirectional 이므로, Forward 방향에서의 state, Backward 방향에서의 state가 모였기 때문에 n_lstm의 2배만큼 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Decoder Embedding 계산하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/f0jLCf5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Gray Scale Image & Dynamic width\n",
    "n_embed = 256\n",
    "decoder_inputs = Input(shape=(None,),name='decoder_inputs')\n",
    "\n",
    "embedding_layer = Embedding(num_classes+1, n_embed)\n",
    "embeded_decoder_inputs = embedding_layer(decoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> n_embed 의 크기는 임의로 정할 수 있습니다. 이번에는 256으로 하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Decoder State Vector($S_{decoder}$) 계산하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/yArrBKh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 초기 state를 넣는 인자를 따로 만들었습니다.<br>\n",
    "이후에 inference Logic을 짤 때, Decoder Logic에서 필요하므로 추가하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state = Input(shape=(n_lstm*2,), name='decoder_state')\n",
    "gru_layer = GRU(n_lstm*2, \n",
    "                name='decoder_gru', \n",
    "                return_sequences=True)\n",
    "\n",
    "states_decoder = gru_layer(embeded_decoder_inputs,\n",
    "                           initial_state=decoder_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Context Vector($C$) 계산하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/cWG0dxO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mbox{(Dot-Attention Score Function) :} \\\\\n",
    "\\omega_{i,j} = score(s_t,h_i) = s_t^{T}h_i\n",
    "$$\n",
    "\n",
    "위와 같은 방식으로 Score을 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Imgur](https://i.imgur.com/ISYwg4N.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mbox{(Score Normalization) :} \\\\\n",
    "\\alpha_{i,j} = \\frac{exp(w_{i,j})}{\\sum_{t=1}^{m}exp(w_{i,t})}\n",
    "$$\n",
    "\n",
    "위와 같은 방식으로 Attention Weght($\\alpha_{i,j}$)을 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/gQsgFve.png)\n",
    "\n",
    " Attention Weight는 각 Decoder의 Time Step 별로, 어떤 Encoder의 State에 대해 가중치를 곱해줄지를 결정하게 됩니다. 그렇게 곱해준 후 더해준 것이, 문맥에 대한 정보인 Context Vector($C_j$)입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Softmax\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        states_encoder = inputs[0]\n",
    "        states_decoder = inputs[1]\n",
    "        \n",
    "        # (1) Calculate Score\n",
    "        expanded_states_encoder = states_encoder[:,None,...] \n",
    "        # >>> (batch size, 1, length of encoder sequence, num hidden)\n",
    "        expanded_states_decoder = states_decoder[...,None,:] \n",
    "        # >>> (batch size, length of decoder sequence, 1, num hidden)            \n",
    "        score = K.sum(expanded_states_encoder * expanded_states_decoder,\n",
    "                          axis=-1)\n",
    "        # >>> (batch size, length of decoder input, length of encoder input)\n",
    "        \n",
    "        # (2) Normalize score\n",
    "        attention = Softmax(axis=-1, name='attention')(score)\n",
    "        \n",
    "        # (3) Calculate Context Vector\n",
    "        context = K.sum(expanded_states_encoder * attention[...,None], axis=2)\n",
    "        # >>> (batch size, length of decoder input, num hidden)\n",
    "        \n",
    "        return context, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotattend = DotAttention()\n",
    "\n",
    "context, attention = dotattend([states_encoder, states_decoder])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) prediction with Softmax\n",
    "\n",
    "![Imgur](https://i.imgur.com/nXJQrCg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clf = 256\n",
    "\n",
    "clf1_layer = Dense(n_clf, activation='relu')\n",
    "clf2_layer = Dense(num_classes+1, activation='softmax')\n",
    "\n",
    "concat_output = concatenate([context, states_decoder],axis=-1, name='concat_output')\n",
    "fc_outputs = TimeDistributed(clf1_layer)(concat_output)\n",
    "predictions = TimeDistributed(clf2_layer, name='output_seqs')(fc_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 Time Step 별로 동일한 Classifier를 적용해주어야 합니다. 이렇게 하기 위해서 Time Step 별로 Layer을 적용해주는 `TimeDistributed`를 이용하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) 모델의 Layer 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "height = 28\n",
    "num_classes = 10\n",
    "n_conv = 16 # the number of Convolution filter\n",
    "n_lstm = 256 # the number of BLSM units\n",
    "n_embed = 256 # the size of embedding vector\n",
    "n_clf = 256 # the number of units in classifier Dense layer\n",
    "\n",
    "# Image Encoder\n",
    "inputs = Input(shape=(height, None, 1),name='images')\n",
    "transposed = K.permute_dimensions(inputs, (0, 2, 1, 3))\n",
    "conv_maps = ConvFeatureExtractor(n_conv,\n",
    "                                 name='feature_extractor')(transposed)\n",
    "feature_seqs = Map2Sequence(name='map_to_sequence')(conv_maps)\n",
    "states_encoder = BLSTMEncoder(n_units=n_lstm)(feature_seqs)    \n",
    "\n",
    "# Embedding Layer\n",
    "decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n",
    "embedding_layer = Embedding(num_classes+1, n_embed)\n",
    "embedding_target = embedding_layer(decoder_inputs)\n",
    "\n",
    "# Text Decoder\n",
    "decoder_state_inputs = Input(shape=(n_lstm*2,), name='decoder_state')\n",
    "gru_layer = GRU(n_lstm*2, \n",
    "                name='decoder_gru', \n",
    "                return_sequences=True)\n",
    "states_decoder = gru_layer(embedding_target,\n",
    "                           initial_state=decoder_state_inputs)\n",
    "\n",
    "# Attention Layer\n",
    "dotattend = DotAttention()\n",
    "context, attention = dotattend([states_encoder, states_decoder])\n",
    "\n",
    "# Classifier Layer\n",
    "clf1_layer = Dense(n_clf, activation='relu')\n",
    "clf2_layer = Dense(num_classes+1, activation='softmax')\n",
    "\n",
    "concat_output = concatenate([context, states_decoder],axis=-1, name='concat_output')\n",
    "fc_outputs = TimeDistributed(clf1_layer)(concat_output)\n",
    "predictions = TimeDistributed(clf2_layer, name='output_seqs')(fc_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (8) 추론 모델과 학습 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training\n",
    "trainer = Model([inputs, \n",
    "                 decoder_inputs,\n",
    "                 decoder_state_inputs], \n",
    "                predictions, name='trainer')\n",
    "\n",
    "# For Inference\n",
    "# - (1) Encoder\n",
    "encoder = Model(inputs, states_encoder, \n",
    "                name='encoder')\n",
    "\n",
    "# - (2) Decoder\n",
    "states_encoder_input = Input((None,n_lstm*2), \n",
    "                             name='states_encoder_input')\n",
    "\n",
    "context, attention = dotattend([states_encoder_input, states_decoder])\n",
    "concat_output = concatenate([context, states_decoder], axis=-1, \n",
    "                            name='concat_output')\n",
    "fc_outputs = TimeDistributed(clf1_layer)(concat_output)\n",
    "predictions = TimeDistributed(clf2_layer, name='output_seqs')(fc_outputs)\n",
    "\n",
    "decoder = Model([states_encoder_input, decoder_inputs, decoder_state_inputs], \n",
    "                [states_decoder, predictions], name='decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (9) 학습 모델 Compile하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.optimizer import AdamW\n",
    "from models.losses import masking_sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "\n",
    "trainer.compile(AdamW(lr=1e-3),\n",
    "                loss={\"output_seqs\":masking_sparse_categorical_crossentropy(-1)},\n",
    "                target_tensors=[y_true])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10) 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks =[]\n",
    "rlrop = ReduceLROnPlateau(factor=0.1,patience=3,min_lr=1e-5)\n",
    "callbacks.append(rlrop)\n",
    "\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit_generator(train_gen,\n",
    "                      epochs=epochs,\n",
    "                      validation_data=valid_gen,\n",
    "                      callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (11) 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,_ = test_gen[0]\n",
    "\n",
    "# Target image \n",
    "target_images = X['images'][:10]\n",
    "\n",
    "# Encoder 결과 계산\n",
    "states_encoder_ = encoder.predict(target_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "batch_size = target_images.shape[0]\n",
    "\n",
    "prev_inputs = np.ones((batch_size,1)) * EOS_TOKEN\n",
    "prev_states = np.zeros((batch_size, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = prev_inputs.copy()\n",
    "while True:\n",
    "    states_decoder_, predictions_ = decoder.predict({\n",
    "        \"states_encoder_input\" : states_encoder_,\n",
    "        \"decoder_inputs\": prev_inputs,\n",
    "        \"decoder_state\": prev_states        \n",
    "    })\n",
    "    prev_states = states_decoder_[:,-1,:]\n",
    "    prev_inputs = np.argmax(predictions_,axis=-1)\n",
    "    \n",
    "    if np.all(prev_inputs == EOS_TOKEN):\n",
    "        break\n",
    "    result = np.concatenate([result,prev_inputs],axis=-1)\n",
    "result = result[:,1:].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, seq in zip(target_images,result):\n",
    "    plt.title(seq)\n",
    "    plt.imshow(image[:,:,0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reference : [Attention based Seq2Seq in Keras](https://wanasit.github.io/attention-based-sequence-to-sequence-in-keras.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
