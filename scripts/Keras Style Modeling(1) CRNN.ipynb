{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "try:\n",
    "    # Font로 자동으로 이미지 만들기\n",
    "    import cairocffi as cairo\n",
    "except:\n",
    "    !pip install cairocffi\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib\n",
    "from scipy import ndimage\n",
    "from functools import partial\n",
    "\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils.dataset import OCRDataset\n",
    "from models.generator import OCRGenerator, KOR2IDX, KOR_CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "assert int(tf.__version__[:1]) < 2.0, \"해당 코드는 1.x에서만 동작합니다.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ MNIST 데이터셋 - CRNN \\]\n",
    "\n",
    "MNIST 데이터셋을 통해 정상적으로 동작하는지를 확인해보도록 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 가져오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import SerializationDataset\n",
    "\n",
    "train_set = SerializationDataset('mnist','train',\n",
    "                                 digit=5,pad_range=(3,10))\n",
    "validation_set = SerializationDataset('mnist','validation',\n",
    "                                      digit=5,pad_range=(3,10))\n",
    "test_set = SerializationDataset('mnist','test',\n",
    "                                digit=(3,8),pad_range=(3,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 Generator 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.generator import DataGenerator\n",
    "\n",
    "train_gen = DataGenerator(train_set, \n",
    "                          batch_size=32)\n",
    "valid_gen = DataGenerator(validation_set, \n",
    "                          batch_size=100, \n",
    "                          shuffle=False)\n",
    "test_gen = DataGenerator(test_set, \n",
    "                         batch_size=500, \n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.layers import ConvFeatureExtractor, Map2Sequence \n",
    "from models.layers import BLSTMEncoder, CTCDecoder\n",
    "from models.losses import ctc_loss\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 28\n",
    "num_classes = 10\n",
    "n_hidden = 16\n",
    "n_lstm = 256\n",
    "K.clear_session()\n",
    "\n",
    "# For Gray Scale Image & Dynamic width\n",
    "inputs = Input(shape=(height, None, 1),name='image')\n",
    "\n",
    "# (batch size, height, width, channels) \n",
    "# -> (batch size, width, height, channels)\n",
    "transposed = K.permute_dimensions(inputs, (0, 2, 1, 3))\n",
    "\n",
    "# CRNN Model\n",
    "conv_maps = ConvFeatureExtractor(n_hidden=n_hidden,\n",
    "                                 name='feature_extractor')(transposed)\n",
    "feature_seqs = Map2Sequence(name='map_to_sequence')(conv_maps)\n",
    "lstm_seqs = BLSTMEncoder(n_units=n_lstm)(feature_seqs)\n",
    "\n",
    "# 우리의 출력 형태는 class 수에 Blank Label을 하나 더해 #classes + 1 만큼을 출력\n",
    "output_seqs = Dense(num_classes+1,\n",
    "                    activation='softmax',\n",
    "                    name='output_seqs')(lstm_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.optimizer import AdamW\n",
    "\n",
    "# 모델 구성하기\n",
    "# (1) 학습 모델 구성하기\n",
    "y_true =  tf.placeholder(shape=(None,None), dtype=tf.int32)\n",
    "trainer = Model(inputs, output_seqs, name='trainer')\n",
    "trainer.compile('adam',\n",
    "                loss={\"output_seqs\":ctc_loss},\n",
    "                target_tensors=[y_true])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### caution\n",
    "\n",
    "`K.ctc_batch_cost`에 이용되는 Input Tensor의 Interface는 아래와 같습니다.\n",
    "\n",
    "* y_true: tensor `(samples, max_string_length)` containing the truth labels.\n",
    "* y_pred: tensor `(samples, time_steps, num_categories)` containing the prediction, or output of the softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) 예측 모델 구성하기\n",
    "predictions = CTCDecoder(beam_width=100)(output_seqs)\n",
    "predictor = Model(inputs, predictions[0], name='predictor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델  학습시키기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit_generator(train_gen,\n",
    "                      epochs=10,\n",
    "                      validation_data=valid_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, true_label in zip(*test_gen[0]):\n",
    "    result = predictor.predict(image[np.newaxis])\n",
    "    predict_seq = \"\".join([str(char) for char in result.ravel()])\n",
    "    plt.title(f'label : {predict_seq}')\n",
    "    plt.imshow(image[:,:,0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ 2. MNIST 데이터셋 - SRN(Sequence Recognition Network) \\]\n",
    "\n",
    "MNIST 데이터셋을 통해 `SRN(Sequence Recognition Network)`는 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 가져오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import SerializationDataset\n",
    "\n",
    "train_set = SerializationDataset('mnist','train',\n",
    "                                 digit=5,pad_range=(3,10))\n",
    "validation_set = SerializationDataset('mnist','validation',\n",
    "                                      digit=5,pad_range=(3,10))\n",
    "test_set = SerializationDataset('mnist','test',\n",
    "                                digit=(3,8),pad_range=(3,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 Generator 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.generator import Seq2SeqGenerator\n",
    "\n",
    "train_gen = Seq2SeqGenerator(train_set, \n",
    "                          batch_size=32)\n",
    "valid_gen = Seq2SeqGenerator(validation_set, \n",
    "                          batch_size=100, \n",
    "                          shuffle=False)\n",
    "test_gen = Seq2SeqGenerator(test_set, \n",
    "                         batch_size=500, \n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 구성하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/1KKH413.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.layers import ConvFeatureExtractor, Map2Sequence \n",
    "from models.layers import BLSTMEncoder, CTCDecoder\n",
    "from models.losses import ctc_loss\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Source Features 계산하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/Ewzjpfa.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 28\n",
    "num_classes = 10\n",
    "n_hidden = 16\n",
    "n_lstm = 256\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# For Gray Scale Image & Dynamic width\n",
    "inputs = Input(shape=(height, None, 1),name='images')\n",
    "\n",
    "# (batch size, height, width, channels) \n",
    "# -> (batch size, width, height, channels)\n",
    "transposed = K.permute_dimensions(inputs, (0, 2, 1, 3))\n",
    "\n",
    "# CRNN Model\n",
    "conv_maps = ConvFeatureExtractor(n_hidden=n_hidden,\n",
    "                                 name='feature_extractor')(transposed)\n",
    "feature_seqs = Map2Sequence(name='map_to_sequence')(conv_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Encoder State Vector($S_{encoder}$) 계산하기\n",
    "\n",
    "![Imgur](https://i.imgur.com/826jsU3.png)\n",
    "\n",
    "$\n",
    "S_{encoder} = [h_1;h_2;h_3;\\cdots;h_k]\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_encoder = BLSTMEncoder(n_units=n_lstm)(feature_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Decoder Embedding 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Gray Scale Image & Dynamic width\n",
    "decoder_inputs = Input(shape=(None,),name='decoder_inputs')\n",
    "\n",
    "embedding_layer = Embedding(num_classes+1,256)\n",
    "embeded_decoder_inputs = embedding_layer(decoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Decoder State Vector($S_{decoder}$) 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_layer = GRU(n_lstm*2, \n",
    "                name='decoder_gru', \n",
    "                return_sequences=True)\n",
    "\n",
    "states_decoder = gru_layer(embeded_decoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Context Vector($C$) 계산하기\n",
    "\n",
    "\n",
    "Code Reference : [Attention based Seq2Seq in Keras](https://wanasit.github.io/attention-based-sequence-to-sequence-in-keras.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Softmax\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs, **kwargs):\n",
    "        states_encoder = inputs[0]\n",
    "        states_decoder = inputs[1]\n",
    "        \n",
    "        # (1) Calculate Score\n",
    "        expanded_states_encoder = states_encoder[:,None,...] \n",
    "        # >>> (batch size, 1, length of encoder sequence, num hidden)\n",
    "        expanded_states_decoder = states_decoder[...,None,:] \n",
    "        # >>> (batch size, length of decoder sequence, 1, num hidden)            \n",
    "        score = K.sum(expanded_states_encoder * expanded_states_decoder,\n",
    "                          axis=-1)\n",
    "        # >>> (batch size, length of decoder input, length of encoder input)\n",
    "        \n",
    "        # (2) Normalize score\n",
    "        attention = Softmax(axis=-1, name='attention')(score)\n",
    "        \n",
    "        # (3) Calculate Context Vector\n",
    "        context = K.sum(expanded_states_encoder * attention[...,None], axis=2)\n",
    "        \n",
    "        return context, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotattend = DotAttention()\n",
    "\n",
    "context, attention = dotattend([states_encoder, states_decoder])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) prediction with Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fc_hidden = 256\n",
    "\n",
    "fc1_layer = Dense(n_fc_hidden, activation='tanh')\n",
    "fc2_layer = Dense(num_classes+1, activation='softmax')\n",
    "\n",
    "concat_output = concatenate([context, states_decoder],axis=-1, name='concat_output')\n",
    "fc_outputs = TimeDistributed(fc1_layer)(concat_output)\n",
    "predictions = TimeDistributed(fc2_layer, name='output_seqs')(fc_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) 모델 전체 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "height = 28\n",
    "num_classes = 10\n",
    "n_conv = 16 # the number of Convolution filter\n",
    "n_lstm = 256 # the number of BLSM units\n",
    "n_fc = 256 # the numbe of final Dense units\n",
    "\n",
    "# For Gray Scale Image & Dynamic width, Target Inputs for Teaching Force\n",
    "inputs = Input(shape=(height, None, 1),name='images')\n",
    "\n",
    "# (batch size, height, width, channels) \n",
    "# -> (batch size, width, height, channels)\n",
    "transposed = K.permute_dimensions(inputs, (0, 2, 1, 3))\n",
    "\n",
    "# CRNN Model\n",
    "conv_maps = ConvFeatureExtractor(n_conv, name='feature_extractor')(transposed)\n",
    "feature_seqs = Map2Sequence(name='map_to_sequence')(conv_maps)\n",
    "\n",
    "# BLSTM Encoder\n",
    "states_encoder = BLSTMEncoder(n_units=n_lstm)(feature_seqs)\n",
    "\n",
    "# Embedding Encoder\n",
    "target_inputs = Input(shape=(None,),name='target_inputs')\n",
    "embedding_layer = Embedding(num_classes+1,256)\n",
    "embedding_target = embedding_layer(target_inputs)\n",
    "\n",
    "# GRU Decoder\n",
    "gru_layer = GRU(n_lstm*2, \n",
    "                name='decoder_gru', \n",
    "                return_sequences=True,\n",
    "                return_state=True)\n",
    "\n",
    "states_decoder, state_last = gru_layer(embedding_target)\n",
    "\n",
    "# Attention Layer\n",
    "dotattend = DotAttention()\n",
    "\n",
    "context, attention = dotattend([states_encoder, states_decoder])\n",
    "\n",
    "# Classifier Layer\n",
    "fc1_layer = Dense(n_fc, activation='tanh')\n",
    "fc2_layer = Dense(num_classes+1, activation='softmax')\n",
    "\n",
    "concat_output = concatenate([context, states_decoder],axis=-1, name='concat_output')\n",
    "fc_outputs = TimeDistributed(fc1_layer)(concat_output)\n",
    "predictions = TimeDistributed(fc2_layer, name='output_seqs')(fc_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) Loss Function 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.optimizer import AdamW\n",
    "from models.losses import masking_sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Model([inputs, target_inputs], predictions, name='trainer')\n",
    "y_true = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "\n",
    "trainer.compile(AdamW(lr=1e-3),\n",
    "                loss={\"output_seqs\":masking_sparse_categorical_crossentropy(-1)},\n",
    "                target_tensors=[y_true])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8) 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit_generator(train_gen, epochs=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (9) 추론 모델(Inference Model) 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Model 구성하기\n",
    "image_encoder = Model(inputs,states_encoder,name='encoder')\n",
    "embedding_layer = Embedding(num_classes+1,256)\n",
    "embedding_target = embedding_layer(target_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Model 구성하기\n",
    "prev_state = Input(shape=(None,n_lstm*2), \n",
    "                   name='prev_state')\n",
    "states_decoder, state_last = gru_layer(embedding_target,\n",
    "                                       initial_state=prev_state)\n",
    "context, attention = dotattend([states_encoder, states_decoder])\n",
    "\n",
    "# Classifier Layer\n",
    "fc1_layer = Dense(n_fc, activation='tanh')\n",
    "fc2_layer = Dense(num_classes+1, activation='softmax')\n",
    "\n",
    "concat_output = concatenate([context, states_decoder], axis=-1, \n",
    "                            name='concat_output')\n",
    "fc_outputs = TimeDistributed(fc1_layer)(concat_output)\n",
    "predictions = TimeDistributed(fc2_layer, name='output_seqs')(fc_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ Synthetic 데이터셋 \\]\n",
    "\n",
    "`cairo` 라이브러리로 작위적으로 만든 한글 이미지로 잘 학습되는 지를 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글 Matplotlib 출력 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 나눔 폰트를 다운받기\n",
    "!apt-get update -qq\n",
    "!apt-get install fonts-nanum* -qq\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "# 2. 나눔 폰트의 위치 가져오기 \n",
    "system_font = fm.findSystemFonts() # 현재 시스템에 설치된 폰트\n",
    "nanum_fonts = [font for font in system_font if \"NanumBarunGothic.ttf\" in font]\n",
    "font_path = nanum_fonts[0] # 설정할 폰트의 경로\n",
    "\n",
    "# 3. 나눔 폰트로 설정하기\n",
    "font_name = fm.FontProperties(fname=font_path, size=10).get_name()\n",
    "plt.rc(\"font\",family=font_name)\n",
    "\n",
    "# 4. 폰트 재설정하기\n",
    "fm._rebuild()\n",
    "\n",
    "# 5. (optional) minus 기호 깨짐 방지\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 가져오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 최소/최대 길이\n",
    "min_words = 4\n",
    "max_words = 8\n",
    "\n",
    "df = pd.read_csv(\"../datasets/wordslist.txt\",names=['word'])\n",
    "df = df.drop_duplicates()\n",
    "df = df[df.word.str.match(r'^[가-힣]+$')]\n",
    "df = df[\n",
    "    df.word.map(\n",
    "        lambda x: (len(x) >= min_words) \n",
    "        and (len(x) <= max_words))]\n",
    "words = df.word.values\n",
    "np.random.shuffle(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCRDataset setting\n",
    "OCRDataset = partial(OCRDataset,\n",
    "                     font_size=24,\n",
    "                     bg_noise=0.2,\n",
    "                     affine_noise=(0.0,0.01),\n",
    "                     color_noise=(0.1,0.3),\n",
    "                     gray_scale=True)\n",
    "\n",
    "dataset = OCRDataset(words)\n",
    "images, labels = dataset[0:3]\n",
    "\n",
    "for image, label in zip(images, labels):\n",
    "    plt.title(label)\n",
    "    plt.imshow(image[:,:,0],cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 Generator 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 군을 기준으로 나누기 \n",
    "# validation words는 5%만 둚\n",
    "valid_words = words[:len(words)*5//100]\n",
    "train_words = words[len(words)*5//100:]\n",
    "\n",
    "valid_set = OCRDataset(valid_words)\n",
    "train_set = OCRDataset(train_words)\n",
    "\n",
    "train_gen = OCRGenerator(train_set, batch_size=64)\n",
    "valid_gen = OCRGenerator(valid_set, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.layers import ConvFeatureExtractor, Map2Sequence \n",
    "from models.layers import BLSTMEncoder, CTCDecoder\n",
    "from models.losses import ctc_loss\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 36\n",
    "num_classes = len(KOR_CHARS)\n",
    "n_hidden = 16\n",
    "n_lstm = 256\n",
    "K.clear_session()\n",
    "\n",
    "# For Gray Scale Image & Dynamic width\n",
    "inputs = Input(shape=(height, None, 1))\n",
    "\n",
    "# (batch size, height, width, channels) \n",
    "# -> (batch size, width, height, channels)\n",
    "transposed = K.permute_dimensions(inputs, (0, 2, 1, 3))\n",
    "\n",
    "# CRNN Model\n",
    "conv_maps = ConvFeatureExtractor(n_hidden=n_hidden,\n",
    "                                 name='feature_extractor')(transposed)\n",
    "feature_seqs = Map2Sequence(name='map_to_sequence')(conv_maps)\n",
    "lstm_seqs = BLSTMEncoder(n_units=n_lstm)(feature_seqs)\n",
    "\n",
    "# 우리의 출력 형태는 class 수에 Blank Label을 하나 더해 #classes + 1 만큼을 출력\n",
    "output_seqs = Dense(num_classes+1,\n",
    "                    activation='softmax',\n",
    "                    name='output_seqs')(lstm_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.optimizer import AdamW\n",
    "\n",
    "# 모델 구성하기\n",
    "# (1) 학습 모델 구성하기\n",
    "y_true =  tf.placeholder(shape=(None,None), dtype=tf.int32)\n",
    "trainer = Model(inputs, output_seqs, name='trainer')\n",
    "trainer.compile('adam',\n",
    "                loss={\"output_seqs\":ctc_loss},\n",
    "                target_tensors=[y_true])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### caution\n",
    "\n",
    "`K.ctc_batch_cost`에 이용되는 Input Tensor의 Interface는 아래와 같습니다.\n",
    "\n",
    "* y_true: tensor `(samples, max_string_length)` containing the truth labels.\n",
    "* y_pred: tensor `(samples, time_steps, num_categories)` containing the prediction, or output of the softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) 예측 모델 구성하기\n",
    "predictions = CTCDecoder(beam_width=100)(output_seqs)\n",
    "predictor = Model(inputs, predictions[0], name='predictor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델  학습시키기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit_generator(train_gen,\n",
    "                      epochs=10,\n",
    "                      validation_data=valid_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, true_label in zip(*test_gen[0]):\n",
    "    result = predictor.predict(image[np.newaxis])\n",
    "    predict_seq = \"\".join([str(char) for char in result.ravel()])\n",
    "    plt.title(f'label : {predict_seq}')\n",
    "    plt.imshow(image[:,:,0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TODO] Attention GRU Modeling\n",
    "\n",
    "### Reference : \n",
    "\n",
    "1. [Neural Machine Translation By Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "2. [Bahdanau Attention 개념 정리](https://hcnoh.github.io/2018-12-11-bahdanau-attention)\n",
    "\n",
    "### GRU의 기본 공식\n",
    "<br>\n",
    "$\n",
    "\\hat y_t = softmax(W_y \\cdot s_t + b_y) \\\\\n",
    "s_t = z_t \\odot s_{t-1} + (1-z_t) \\odot \\bar s_t \\\\\n",
    "z_t = \\sigma(W_z y_{t-1} + U_z s_{t-1} + b_z) \\\\\n",
    "r_t = \\sigma(W_r y_{t-1} + U_r s_{t-1} + b_r) \\\\\n",
    "\\bar s_t = tanh(W_s y_{t-1} + U_s(r_t \\odot s_{t-1}) + b_s) \\\\\n",
    "$\n",
    "\n",
    "Attention 메커니즘을 활용하여 위의 연산들을 재정의하면 아래와 같이 정리할 수 있다. <br>\n",
    "<br>\n",
    "$\n",
    "\\hat y_t = softmax(W_y \\cdot s_t + b_y) \\\\\n",
    "s_t = z_t \\odot s_{t-1} + (1-z_t) \\odot \\bar s_t \\\\\n",
    "z_t = \\sigma(W_z y_{t-1} + U_z s_{t-1} + C_z c_t + b_z) \\\\\n",
    "r_t = \\sigma(W_r y_{t-1} + U_r s_{t-1} + C_r c_t + b_r) \\\\\n",
    "\\bar s_t = tanh(W_s y_{t-1} + U_s(r_t \\odot s_{t-1}) + C_s c_t + b_s) \\\\\n",
    "$\n",
    "\n",
    "GRU 모델 및 기본 RNN 모델에서의 Context Vector의 활용을 살펴보면 다음의 특징을 파악할 수 있습니다. Context Vector $c_t$는 RNN의 입력으로 사용되는 $y_{t-1}$ 과 함께 등장하며 함께 임베딩 공간에 뿌려져서 더해지는 방식으로 활용됩니다. 즉, 간단하게 정리하자면 $Wy_{t-1}$ 대신 $W y_{t-1} + C c_t$가 된다는 것이다. 이건 RNN 입력을 y_{t-1} 단독으로 사용하는 것이 아니라 Context Vector C_t와 Concatenation하여 사용하는 것과 같은 의미입니다. 이걸 수식으로 정리하면 다음과 같습니다. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU, Embedding\n",
    "from tensorflow.keras.layers import Layer, GRUCell, LSTMCell, RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionCell(Layer):\n",
    "    \"\"\"\n",
    "    Robust Scene Text Recognition with Automatic Rectification에서 나오는 \n",
    "    <Attend> Network에 대한 Module Class\n",
    "    \n",
    "    Reference : \n",
    "    \n",
    "    BLSTM Encoder Sequence에서 우리가 원하는 Text Sequence으로 바꾸기 위해, \n",
    "    BLSTM 부분에서 어떤 것들이 필요한 것인지를 파악\n",
    "    \"\"\"\n",
    "    def __init__(self, n_units, **kwargs):\n",
    "        self.state_size = n_units\n",
    "        self.input_proj_layer = Dense(n_units, use_bias=False, \n",
    "                                      name='input_project')\n",
    "        self.state_proj_layer = Dense(n_units,\n",
    "                                      name='state_project')\n",
    "        self.score_layer = Dense(1, use_bias=False,\n",
    "                                 name='score')\n",
    "        self.gru_layer = GRUCell(n_units)        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, inputs, states):\n",
    "        # (batch size, time step, hidden size) -> (batch size, time step, hidden size)\n",
    "        h_proj = self.input_proj_layer(inputs)\n",
    "        # (batch size, hidden size) -> (batch size, 1, hidden size)\n",
    "        s_proj = self.state_proj_layer(states[0])\n",
    "        s_proj = s_proj[:,None,:]\n",
    "        \n",
    "        # (batch size, time step, hidden size) \n",
    "        # -> (batch size, time step, 1) -> (batch size, time step)\n",
    "        score = self.score_layer(K.tanh(h_proj+s_proj))\n",
    "        score = score[:,:,0]\n",
    "        \n",
    "        alpha = K.softmax(score)[:,:,None]\n",
    "        context = K.sum(inputs*alpha, axis=1)\n",
    "                \n",
    "        alpha = tf.identity(alpha, name='alpha')        \n",
    "        context = tf.identity(context, name='context')\n",
    "        print(\"Context : \", context.shape)\n",
    "        print(\"State[0]   : \", self.gru_layer(context, states)[0].shape)\n",
    "        print(\"State[1]   : \", self.gru_layer(context, states)[1])\n",
    "        return self.gru_layer(context, states)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# For Gray Scale Image & Dynamic width\n",
    "inputs = Input(shape=(height, None, 1))\n",
    "labels = Input(shape=(None,))\n",
    "\n",
    "x = Embedding(num_classes+1,256)\n",
    "y = x(labels)\n",
    "\n",
    "# (batch size, height, width, channels) -> (batch size, width, height, channels)\n",
    "transposed = K.permute_dimensions(inputs, (0, 2, 1, 3))\n",
    "\n",
    "# CRNN Model\n",
    "conv_maps = ConvFeatureExtractor(name='feature_extractor')(transposed)\n",
    "feature_seqs = Map2Sequence(name='map_to_sequence')(conv_maps)\n",
    "blstm_seqs = BLSTMEncoder(n_units=256,name='blstm_encoder')(feature_seqs)\n",
    "# attend_seqs = RNN(AttentionCell(n_units=256*2),\n",
    "#                   return_sequences=True, \n",
    "#                   name='GRU_Attention')(blstm_seqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Attention, AdditiveAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdditiveAttention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstm_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attend_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_seqs = Dense(num_classes+1, activation='softmax')(attend_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_seqs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
