{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "try:\n",
    "    # Font로 자동으로 이미지 만들기\n",
    "    import cairocffi as cairo\n",
    "except:\n",
    "    !pip install cairocffi\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib\n",
    "from scipy import ndimage\n",
    "from functools import partial\n",
    "\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils.dataset import OCRDataset\n",
    "from models.generator import OCRGenerator, KOR2IDX, KOR_CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "assert int(tf.__version__[:1]) < 2.0, \"해당 코드는 1.x에서만 동작합니다.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ MNIST 데이터셋 \\]\n",
    "\n",
    "MNIST 데이터셋을 통해 정상적으로 동작하는지를 확인해보도록 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 가져오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import SerializationDataset\n",
    "\n",
    "train_set = SerializationDataset('mnist','train',\n",
    "                                 digit=5,pad_range=(3,10))\n",
    "validation_set = SerializationDataset('mnist','validation',\n",
    "                                      digit=5,pad_range=(3,10))\n",
    "test_set = SerializationDataset('mnist','test',\n",
    "                                digit=(3,8),pad_range=(3,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 Generator 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.generator import DataGenerator\n",
    "\n",
    "train_gen = DataGenerator(train_set, \n",
    "                          batch_size=32)\n",
    "valid_gen = DataGenerator(validation_set, \n",
    "                          batch_size=100, \n",
    "                          shuffle=False)\n",
    "test_gen = DataGenerator(test_set, \n",
    "                         batch_size=500, \n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.layers import ConvFeatureExtractor, Map2Sequence \n",
    "from models.layers import BLSTMEncoder, CTCDecoder\n",
    "from models.losses import ctc_loss\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 28\n",
    "num_classes = 10\n",
    "n_hidden = 16\n",
    "n_lstm = 128\n",
    "K.clear_session()\n",
    "\n",
    "# For Gray Scale Image & Dynamic width\n",
    "inputs = Input(shape=(height, None, 1))\n",
    "\n",
    "# (batch size, height, width, channels) -> (batch size, width, height, channels)\n",
    "transposed = K.permute_dimensions(inputs, (0, 2, 1, 3))\n",
    "\n",
    "# CRNN Model\n",
    "conv_maps = ConvFeatureExtractor(n_hidden=16, name='feature_extractor')(transposed)\n",
    "feature_seqs = Map2Sequence(name='map_to_sequence')(conv_maps)\n",
    "lstm_seqs = BLSTMEncoder(n_units=n_lstm)(feature_seqs)\n",
    "# 우리의 출력 형태는 class 수에 Blank Label을 하나 더해 #classes + 1 만큼을 출력\n",
    "output_seqs = Dense(num_classes+1, activation='softmax', name='output_seqs')(lstm_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.optimizer import AdamW\n",
    "\n",
    "# 모델 구성하기\n",
    "# (1) 학습 모델 구성하기\n",
    "y_true =  tf.placeholder(shape=(None,None), dtype=tf.int32)\n",
    "trainer = Model(inputs, output_seqs, name='trainer')\n",
    "trainer.compile(AdamW(lr=1e-2),\n",
    "                loss={\"output_seqs\":ctc_loss},\n",
    "                target_tensors=[y_true])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### caution\n",
    "\n",
    "`K.ctc_batch_cost`에 이용되는 Input Tensor의 Interface는 아래와 같습니다.\n",
    "\n",
    "* y_true: tensor `(samples, max_string_length)` containing the truth labels.\n",
    "* y_pred: tensor `(samples, time_steps, num_categories)` containing the prediction, or output of the softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) 예측 모델 구성하기\n",
    "predictions = CTCDecoder(beam_width=100)(output_seqs)\n",
    "predictor = Model(inputs, predictions[0], name='predictor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델  학습시키기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit_generator(train_gen,\n",
    "                      epochs=10,\n",
    "                      validation_data=valid_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, true_label in zip(*test_gen[0]):\n",
    "    result = predictor.predict(image[np.newaxis])\n",
    "    predict_seq = \"\".join([str(char) for char in result.ravel()])\n",
    "    plt.title(f'label : {predict_seq}')\n",
    "    plt.imshow(image[:,:,0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\[ Synthetic 데이터셋 \\]\n",
    "\n",
    "`cairo` 라이브러리로 작위적으로 만든 한글 이미지로 잘 학습되는 지를 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글 Matplotlib 출력 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 나눔 폰트를 다운받기\n",
    "!apt-get update -qq\n",
    "!apt-get install fonts-nanum* -qq\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "# 2. 나눔 폰트의 위치 가져오기 \n",
    "system_font = fm.findSystemFonts() # 현재 시스템에 설치된 폰트\n",
    "nanum_fonts = [font for font in system_font if \"NanumBarunGothic.ttf\" in font]\n",
    "font_path = nanum_fonts[0] # 설정할 폰트의 경로\n",
    "\n",
    "# 3. 나눔 폰트로 설정하기\n",
    "font_name = fm.FontProperties(fname=font_path, size=10).get_name()\n",
    "plt.rc(\"font\",family=font_name)\n",
    "\n",
    "# 4. 폰트 재설정하기\n",
    "fm._rebuild()\n",
    "\n",
    "# 5. (optional) minus 기호 깨짐 방지\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 최소/최대 길이\n",
    "min_words = 4\n",
    "max_words = 8\n",
    "\n",
    "# OCRDataset setting\n",
    "OCRDataset = partial(OCRDataset,\n",
    "                     font_size=24,\n",
    "                     bg_noise=0.2,\n",
    "                     affine_noise=(0.0,0.03),\n",
    "                     color_noise=(0.3,0.6),\n",
    "                     gray_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/wordslist.txt\",names=['word'])\n",
    "df = df.drop_duplicates()\n",
    "df = df[df.word.str.match(r'^[가-힣]+$')]\n",
    "df = df[\n",
    "    df.word.map(\n",
    "        lambda x: (len(x) >= min_words) \n",
    "        and (len(x) <= max_words))]\n",
    "words = df.word.values\n",
    "np.random.shuffle(words)\n",
    "\n",
    "dataset = OCRDataset(words)\n",
    "images, labels = dataset[0:3]\n",
    "\n",
    "for image, label in zip(images, labels):\n",
    "    plt.title(label)\n",
    "    plt.imshow(image[:,:,0],cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 군을 기준으로 나누기 \n",
    "# validation words는 5%만 둚\n",
    "\n",
    "valid_words = words[:len(words)*5//100]\n",
    "train_words = words[len(words)*5//100:]\n",
    "\n",
    "valid_set = OCRDataset(valid_words)\n",
    "train_set = OCRDataset(train_words)\n",
    "\n",
    "train_gen = OCRGenerator(train_set, batch_size=16, eos_token=True)\n",
    "valid_gen = OCRGenerator(valid_set, batch_size=16, eos_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.keras_crnn import ConvFeatureExtractor, Map2Sequence \n",
    "from models.keras_crnn import BLSTMEncoder, CTCDecoder, ctc_loss\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 36\n",
    "num_classes = len(KOR_CHARS)\n",
    "K.clear_session()\n",
    "\n",
    "# For Gray Scale Image & Dynamic width\n",
    "inputs = Input(shape=(height, None, 1))\n",
    "\n",
    "# Preprocessing : (0,255.) -> (-1,1.)\n",
    "preprocess = (inputs - 127.5) / 255.\n",
    "\n",
    "# (batch size, height, width, channels) -> (batch size, width, height, channels)\n",
    "transposed = K.permute_dimensions(preprocess, (0, 2, 1, 3))\n",
    "\n",
    "# CRNN Model\n",
    "conv_maps = ConvFeatureExtractor(name='feature_extractor')(transposed)\n",
    "feature_seqs = Map2Sequence(name='map_to_sequence')(conv_maps)\n",
    "lstm_seqs = BLSTMEncoder()(feature_seqs)\n",
    "# 우리의 출력 형태는 class 수에 Blank Label을 하나 더해 #classes + 1 만큼을 출력\n",
    "output_seqs = Dense(num_classes+1, activation='softmax', name='output_seqs')(lstm_seqs)\n",
    "\n",
    "# 모델 구성하기\n",
    "# (1) 학습 모델 구성하기\n",
    "y_true =  tf.placeholder(shape=(None,None),dtype=tf.int32)\n",
    "trainer = Model(inputs, output_seqs, name='trainer')\n",
    "trainer.compile('adam',\n",
    "                loss={\"output_seqs\":ctc_loss},\n",
    "                target_tensors=[y_true])\n",
    "\n",
    "# Caution :\n",
    "# CTC Loss의 경우 y_pred와 y_true의 형태가 다릅니다.\n",
    "# Keras는 y_pred과 y_true의 shape가 동일하다고 가정하기 때문에, 다른 경우\n",
    "# compile 시 target_tensor에 직접 y_true의 형태를 지정해주어야 합니다.\n",
    "\n",
    "# (2) 예측 모델 구성하기\n",
    "predictions = CTCDecoder(beam_width=100)(output_seqs)\n",
    "predictor = Model(inputs, predictions, name='predictor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit_generator(train_gen,\n",
    "                      epochs=10,\n",
    "                      validation_data=valid_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TODO] Attention GRU Modeling\n",
    "\n",
    "---\n",
    "\n",
    "### Reference : \n",
    "\n",
    "1. [Neural Machine Translation By Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "2. [Bahdanau Attention 개념 정리](https://hcnoh.github.io/2018-12-11-bahdanau-attention)\n",
    "\n",
    "### GRU의 기본 공식\n",
    "<br>\n",
    "$\n",
    "\\hat y_t = softmax(W_y \\cdot s_t + b_y) \\\\\n",
    "s_t = z_t \\odot s_{t-1} + (1-z_t) \\odot \\bar s_t \\\\\n",
    "z_t = \\sigma(W_z y_{t-1} + U_z s_{t-1} + b_z) \\\\\n",
    "r_t = \\sigma(W_r y_{t-1} + U_r s_{t-1} + b_r) \\\\\n",
    "\\bar s_t = tanh(W_s y_{t-1} + U_s(r_t \\odot s_{t-1}) + b_s) \\\\\n",
    "$\n",
    "\n",
    "Attention 메커니즘을 활용하여 위의 연산들을 재정의하면 아래와 같이 정리할 수 있다. <br>\n",
    "<br>\n",
    "$\n",
    "\\hat y_t = softmax(W_y \\cdot s_t + b_y) \\\\\n",
    "s_t = z_t \\odot s_{t-1} + (1-z_t) \\odot \\bar s_t \\\\\n",
    "z_t = \\sigma(W_z y_{t-1} + U_z s_{t-1} + C_z c_t + b_z) \\\\\n",
    "r_t = \\sigma(W_r y_{t-1} + U_r s_{t-1} + C_r c_t + b_r) \\\\\n",
    "\\bar s_t = tanh(W_s y_{t-1} + U_s(r_t \\odot s_{t-1}) + C_s c_t + b_s) \\\\\n",
    "$\n",
    "\n",
    "GRU 모델 및 기본 RNN 모델에서의 Context Vector의 활용을 살펴보면 다음의 특징을 파악할 수 있습니다. Context Vector $c_t$는 RNN의 입력으로 사용되는 $y_{t-1}$ 과 함께 등장하며 함께 임베딩 공간에 뿌려져서 더해지는 방식으로 활용됩니다. 즉, 간단하게 정리하자면 $Wy_{t-1}$ 대신 $W y_{t-1} + C c_t$가 된다는 것이다. 이건 RNN 입력을 y_{t-1} 단독으로 사용하는 것이 아니라 Context Vector C_t와 Concatenation하여 사용하는 것과 같은 의미입니다. 이걸 수식으로 정리하면 다음과 같습니다. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU, Embedding\n",
    "from tensorflow.keras.layers import Layer, GRUCell, LSTMCell, RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionCell(Layer):\n",
    "    \"\"\"\n",
    "    Robust Scene Text Recognition with Automatic Rectification에서 나오는 \n",
    "    <Attend> Network에 대한 Module Class\n",
    "    \n",
    "    Reference : \n",
    "    \n",
    "    BLSTM Encoder Sequence에서 우리가 원하는 Text Sequence으로 바꾸기 위해, \n",
    "    BLSTM 부분에서 어떤 것들이 필요한 것인지를 파악\n",
    "    \"\"\"\n",
    "    def __init__(self, n_units, **kwargs):\n",
    "        self.state_size = n_units\n",
    "        self.input_proj_layer = Dense(n_units, use_bias=False, \n",
    "                                      name='input_project')\n",
    "        self.state_proj_layer = Dense(n_units,\n",
    "                                      name='state_project')\n",
    "        self.score_layer = Dense(1, use_bias=False,\n",
    "                                 name='score')\n",
    "        self.gru_layer = GRUCell(n_units)        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, inputs, states):\n",
    "        # (batch size, time step, hidden size) -> (batch size, time step, hidden size)\n",
    "        h_proj = self.input_proj_layer(inputs)\n",
    "        # (batch size, hidden size) -> (batch size, 1, hidden size)\n",
    "        s_proj = self.state_proj_layer(states[0])\n",
    "        s_proj = s_proj[:,None,:]\n",
    "        \n",
    "        # (batch size, time step, hidden size) \n",
    "        # -> (batch size, time step, 1) -> (batch size, time step)\n",
    "        score = self.score_layer(K.tanh(h_proj+s_proj))\n",
    "        score = score[:,:,0]\n",
    "        \n",
    "        alpha = K.softmax(score)[:,:,None]\n",
    "        context = K.sum(inputs*alpha, axis=1)\n",
    "                \n",
    "        alpha = tf.identity(alpha, name='alpha')        \n",
    "        context = tf.identity(context, name='context')\n",
    "        print(\"Context : \", context.shape)\n",
    "        print(\"State[0]   : \", self.gru_layer(context, states)[0].shape)\n",
    "        print(\"State[1]   : \", self.gru_layer(context, states)[1])\n",
    "        return self.gru_layer(context, states)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# For Gray Scale Image & Dynamic width\n",
    "inputs = Input(shape=(height, None, 1))\n",
    "labels = Input(shape=(None,))\n",
    "\n",
    "x = Embedding(num_classes+1,256)\n",
    "y = x(labels)\n",
    "\n",
    "# (batch size, height, width, channels) -> (batch size, width, height, channels)\n",
    "transposed = K.permute_dimensions(inputs, (0, 2, 1, 3))\n",
    "\n",
    "# CRNN Model\n",
    "conv_maps = ConvFeatureExtractor(name='feature_extractor')(transposed)\n",
    "feature_seqs = Map2Sequence(name='map_to_sequence')(conv_maps)\n",
    "blstm_seqs = BLSTMEncoder(n_units=256,name='blstm_encoder')(feature_seqs)\n",
    "# attend_seqs = RNN(AttentionCell(n_units=256*2),\n",
    "#                   return_sequences=True, \n",
    "#                   name='GRU_Attention')(blstm_seqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Attention, AdditiveAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdditiveAttention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blstm_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attend_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_seqs = Dense(num_classes+1, activation='softmax')(attend_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_seqs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
